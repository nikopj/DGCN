  0%|          | 0/432 [00:00<?, ?it/s]  6%|▋         | 28/432 [00:00<00:01, 276.81it/s] 14%|█▍        | 61/432 [00:00<00:01, 289.10it/s] 22%|██▏       | 93/432 [00:00<00:01, 296.62it/s] 29%|██▉       | 125/432 [00:00<00:01, 303.02it/s] 37%|███▋      | 158/432 [00:00<00:00, 308.32it/s] 44%|████▍     | 190/432 [00:00<00:00, 311.04it/s] 51%|█████▏    | 222/432 [00:00<00:00, 312.14it/s] 59%|█████▉    | 254/432 [00:00<00:00, 314.33it/s] 66%|██████▋   | 287/432 [00:00<00:00, 317.25it/s] 74%|███████▍  | 319/432 [00:01<00:00, 316.32it/s] 81%|████████▏ | 351/432 [00:01<00:00, 316.56it/s] 89%|████████▊ | 383/432 [00:01<00:00, 316.67it/s] 96%|█████████▌| 415/432 [00:01<00:00, 317.07it/s]100%|██████████| 432/432 [00:01<00:00, 315.49it/s]
  0%|          | 0/68 [00:00<?, ?it/s] 31%|███       | 21/68 [00:00<00:00, 202.97it/s] 60%|██████    | 41/68 [00:00<00:00, 201.92it/s] 91%|█████████ | 62/68 [00:00<00:00, 202.23it/s]100%|██████████| 68/68 [00:00<00:00, 201.79it/s]
  0%|          | 0/12 [00:00<?, ?it/s]100%|██████████| 12/12 [00:00<00:00, 412.78it/s]
TRAIN-E1:   0%|          | 0/54 [00:00<?, ?it/s]Traceback (most recent call last):
  File "train.py", line 210, in <module>
    main(args)
  File "train.py", line 33, in main
    epoch_fun   = lambda epoch_num: saveArgs(args, epoch_num))
  File "train.py", line 71, in fit
    output = nn.parallel.data_parallel(model, noisy_batch)
  File "/home/npj226/py3env/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py", line 210, in data_parallel
    outputs = parallel_apply(replicas, inputs, module_kwargs, used_device_ids)
  File "/home/npj226/py3env/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py", line 85, in parallel_apply
    output.reraise()
  File "/home/npj226/py3env/lib/python3.7/site-packages/torch/_utils.py", line 395, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/home/npj226/py3env/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py", line 60, in _worker
    output = module(*input, **kwargs)
  File "/home/npj226/py3env/lib/python3.7/site-packages/torch/nn/modules/module.py", line 550, in __call__
    result = self.forward(*input, **kwargs)
  File "/scratch/npj226/DGCN/net.py", line 59, in forward
    z = z + self.LPF[i](z)
  File "/home/npj226/py3env/lib/python3.7/site-packages/torch/nn/modules/module.py", line 550, in __call__
    result = self.forward(*input, **kwargs)
  File "/scratch/npj226/DGCN/net.py", line 98, in forward
    x = self.GConv[i](x, edge)
  File "/home/npj226/py3env/lib/python3.7/site-packages/torch/nn/modules/module.py", line 550, in __call__
    result = self.forward(*input, **kwargs)
  File "/scratch/npj226/DGCN/net.py", line 121, in forward
    hNL  = self.NLAgg(h, edge)
  File "/home/npj226/py3env/lib/python3.7/site-packages/torch/nn/modules/module.py", line 550, in __call__
    result = self.forward(*input, **kwargs)
  File "/scratch/npj226/DGCN/net.py", line 170, in forward
    thetaL = self.FCL(theta).reshape(B0, self.Cout, self.rank)
  File "/home/npj226/py3env/lib/python3.7/site-packages/torch/nn/modules/module.py", line 550, in __call__
    result = self.forward(*input, **kwargs)
  File "/scratch/npj226/DGCN/net.py", line 201, in forward
    return F.conv1d(xpad, self.weight).view(-1,self.Cout)
RuntimeError: CUDA out of memory. Tried to allocate 320.00 MiB (GPU 0; 11.17 GiB total capacity; 10.44 GiB already allocated; 115.31 MiB free; 10.75 GiB reserved in total by PyTorch)

TRAIN-E1:   0%|          | 0/54 [00:03<?, ?it/s]
